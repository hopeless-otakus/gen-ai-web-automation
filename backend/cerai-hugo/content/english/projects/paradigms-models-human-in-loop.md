---

title: "Paradigms, Interpretable Models, Algorithms for AI-based Human in the Loop Learning"
image: "https://rbcdsai.iitm.ac.in/images/research/Paradigms-Interpretable-Models-and-Algorithms-for-AI-based-Human-in-the-Loop-Learning.jpg"
draft: false
researchers: ["Arun Rajkumar, Harish Guruprasad", "Chandrasekhar L"]
filters: [making-ai-understandable]
tags: ["human-in-loop", "explainable-ai"]
reslinks: ["https://ioe.iitm.ac.in/project/deployable-ai/"]
reslinktitles: ["Paradigms, Interpretable Models, Algorithms for AI-based Human in the Loop Learning: IITM"]
---

Artificial intelligence/Data science systems with humans-in-the-loop (HIL) are increasing by the day with applications covering a broad spectrum of domains ranging from education to e-commerce. AI for HIL systems involves two kinds of continual learners namely humans and computers. The success of these systems critically depends on the interaction between these two learners. Towards this, we propose to investigate the following fundamental questions in AI-based HIL systems.

-- The data is typically not stationary because human behaviour changes over time. While some of the non-stationarity can be addressed via shifting distribution, an alternate yet important aspect is that humans are (possibly subconsciously) modifying their behaviour in a potentially non-stochastic or strategic (game-theoretic) manner to adapt to the underlying system experience. We are interested in investigating the fundamental paradigms of simultaneous learning in dynamic HIL systems.

-- In many scenarios of HIL systems, either data is missing or only could be obtained via additional interventions. For instance, an important application we are interested in is MOOCs in the Indian context where a large part of the data (e.g. video viewing time data, etc) is available in aggregate and non-user specific form. We are interested in understanding principled approaches to handle missing/partial/aggregate data in HIL systems to prescribe user-specific interventions.

-- A major challenge is to develop explainable HIL based continual learning systems where the actions taken by the system should be interpretable. This has been a major bottleneck for the sophisticated state of the art deep learning-based reinforcement learning systems. For instance, in the application of interest to us i.e., MOOCs in the Indian context, if the system chooses a specific intervention to improve user retention, then it should back it up with easy to understand the reasoning for making the decision. We are interested in developing interpretable models for ser interventions in AI-based HIL systems. As an application of the above proposal, we are interested in looking at MOOCs in the Indian context (NPTEL). With our general approach, we hope the following interesting points can be addressed.

-- Analyse NPTEL datasets and explain the most common reasons for dropouts and draw actionable insights.

-- Predict when a student/user would drop out by analysing her viewing activity.

-- Incorporate potentially personalized intervention mechanisms (could be as simple as automatically linking relevant prerequisite videos at the right time in a stream, creating an automated list of user-specific FAQ, automatically splitting a video into logical chunks, etc.) and observe retention rates.

-- Suggest potential widgets that NPTEL might incorporate for better feedback mechanisms (for instance, a user might be allowed to choose parts of the video which she does not understand well, etc.)

Several existing studies for user retention in online courses tackle the issue either from a sociological perspective or are too aligned to the western context. We wish to develop a first of its kind study that incorporates fundamental data science paradigms for dynamic AI-based HIL systems as building blocks and address potential challenges specific to the Indian context in such systems.