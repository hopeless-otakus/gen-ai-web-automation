---
title: "Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation"
image: "/images/images.jpeg"
researchimg: "/images/deception4.png"
reslinks: ["https://arxiv.org/abs/2405.04325"]
tags: ["ethics", "explainable-ai"]
reslinktitles: ["Deception in Reinforced Autonomous Agents: The Unconventional Rabbit Hat Trick in Legislation: arxiv.org"]
collaborators: ["/images/princeton.png", "/images/stanford.webp"]
filters: [safe-ai]
draft: false
---

Recent developments in large language models (LLMs), while offering a powerful foundation for developing natural language agents, raise safety concerns about them and the autonomous agents built upon them. Deception is one potential capability of AI agents of particular concern, which we refer to as an act or statement that misleads, hides the truth, or promotes a belief that is not true in its entirety or in part. We move away from the conventional understanding of deception through straight-out lying, making objective selfish decisions, or giving false information, as seen in previous AI safety research.



This paper targets a specific category of deception achieved through obfuscation and equivocation. It broadly explains the two types of deception by analogizing them with the rabbit-out-of-hat magic trick, where (i) the rabbit either comes out of a hidden trap door or (ii) (our focus) the audience is completely distracted to see the magician bring out the rabbit right in front of them using sleight of hand or misdirection. Our novel testbed framework displays intrinsic deception capabilities of LLM agents in a goal-driven environment when directed to be deceptive in their natural language generations in a two-agent adversarial dialogue system built upon the legislative task of "lobbying" for a bill. 
<br>

<img src="/images/deception3.png">

<br>

<br>

<img src="/images/deception2.png">

<br>

Along the lines of a goal-driven environment, we show developing deceptive capacity through a reinforcement learning setup, building it around the theories of language philosophy and cognitive psychology. We find that the lobbyist agent increases its deceptive capabilities by ~ 40% (relative) through subsequent reinforcement trials of adversarial interactions, and our deception detection mechanism shows a detection capability of up to 92%. Our results highlight potential issues in agent-human interaction, with agents potentially manipulating humans towards its programmed end-goal. 

<br>

<img src="/images/deception1.png">

<br>